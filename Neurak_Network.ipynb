{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Neural, master=local[*]) created by __init__ at <ipython-input-1-3ea5fb6248f2>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3ea5fb6248f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Neural\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Neural, master=local[*]) created by __init__ at <ipython-input-1-3ea5fb6248f2>:7 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Neural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.43, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer=2\n",
    "output_layer=10\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "#gradient decent\n",
    "eta=0.01\n",
    "reg_lambda=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(X,y,nn_hdim, num_passes, print_loss):\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    W1,b1,W2,b2=initialize_weights(nn_input_dim, nn_hdim,nn_output_dim)\n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0,num_passes):\n",
    "        #Forward propagation   \n",
    "        k=0\n",
    "        for x in X:\n",
    "            \n",
    "            #z1,a1,z2,probs=forward(W1,b1,W2,b2,x)\n",
    "        \n",
    "            dW1,db1,dW2,db2=feedforward(W1,b1,W2,b2,x,y[k])\n",
    "#             # Backpropagation\n",
    "#             delta3 = probs\n",
    "#             #print(len(x))\n",
    "#             delta3[range(len(x)-1), y[k]] -= 1\n",
    "#             dW2 = (a1.T).dot(delta3)\n",
    "#             db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "#             delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "#             m=np.reshape(x,(2,1))\n",
    "#             dW1 = np.dot(m, delta2)\n",
    "#             db1 = np.sum(delta2, axis=0)\n",
    "            k=k+1\n",
    "            # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "        W1 += -eta * dW1\n",
    "        b1 += -eta * db1\n",
    "        W2 += -eta * dW2\n",
    "        b2 += -eta * db2\n",
    "            \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            \n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model,X,y)))  \n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "num_examples = len(X_train)\n",
    "model = build_model(X_train,y_train,40,100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=np.array(([[1],[2]]))\n",
    "y_pred=[]\n",
    "for i in X_train:\n",
    "    y_pred.append(predict(model,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85087719298245612"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 40)\n"
     ]
    }
   ],
   "source": [
    "y_pred.append(predict(model,np.array([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    y=0\n",
    "    for i in x:\n",
    "        x[y] = float(i)\n",
    "        y+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile(\"C:/Users/saika/Desktop/files/TestFiles/neural.csv\").filter(lambda x:x[0]!=\",\").map(lambda x:x.split(\",\")[1:])\n",
    "trainRDD = trainRDD.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(model,X,y):\n",
    "    W1, W2 = model['W1'],  model['W2']\n",
    "    z1,a1,z2,probs=forward(W1,W2,X)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim_input, dim_hidden,dim_output):\n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    w1 = np.random.randn(dim_input, dim_hidden) / np.sqrt(dim_input)\n",
    "    w0 = np.random.randn(dim_hidden, dim_output) / np.sqrt(dim_hidden)\n",
    "    return w1,w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_scores = np.exp(z)\n",
    "    return (exp_scores / np.sum(exp_scores, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(W1,W2,X):\n",
    "    a1 = np.column_stack(( X))\n",
    "    print(np.shape(a1))\n",
    "    # hidden activations\n",
    "    z1 = a1.dot(W1)\n",
    "    a1 = np.tanh(z1)\n",
    "    #a1 = np.vstack(( sigmoid(z1)))\n",
    "    # output activations\n",
    "    z2 = a1.dot(W2)\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return z1,a1,z2,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, W2= model['w1'], model['w2']\n",
    "    # Forward propagation\n",
    "    z1,a1,z2,probs=forward(W1,W2,x)\n",
    "    return np.argmax(probs,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feedforward(W1,W2,x,y):\n",
    "    a1 = np.column_stack((x))\n",
    "    print(np.shape(a1))\n",
    "    # hidden activations\n",
    "    z1 = a1.dot(W1)\n",
    "    a1 = np.tanh(z1)\n",
    "    #a1 = np.vstack(( sigmoid(z1)))\n",
    "\n",
    "    # output activations\n",
    "    z2 = a1.dot(W2)\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    #back Propagation\n",
    "    delta3 = probs\n",
    "            #print(len(x))\n",
    "#     delta3 = delta3.transpose()\n",
    "    delta3[0][y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "    m=np.reshape(np.array(x),(3012,1))\n",
    "    dW1 = np.dot(m, delta2)\n",
    "    return np.array([dW1,dW2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Network(w1,w2, train_set, max_iter):\n",
    "    Lambda = 10.01 # Regularization parameter (to avoid overfit issue)\n",
    "    m = train_set.count()\n",
    "    \n",
    "    for ik in range(max_iter):\n",
    "        eval_res = train_set.map(lambda x: feedforward(w1,w2,x[0:-1],x[-1]))\n",
    "        \n",
    "        # calculate derivation of weights via average bp results\n",
    "        average_eval = eval_res.reduce(add) / train_set.count()\n",
    "        dw1 = average_eval[0]\n",
    "        dw2 = average_eval[1]\n",
    "        \n",
    "        \n",
    "        # for all the weights, you should not apply regulization on the first column since they are for bias\n",
    "        for i in range(w1.shape[0]):\n",
    "            w1[i, 0] = w1[i, 0] - dw1[i, 0]\n",
    "        for i in range(w1.shape[0]):\n",
    "            for j in range(1, w1.shape[1]):\n",
    "                # here learn rate is 1.0, Lambda is the Regularization parameter\n",
    "                w1[i, j] = w1[i, j] - (dw1[i, j] + (Lambda/m) * w1[i, j])\n",
    "        \n",
    "        for i in range(w2.shape[0]):\n",
    "            w2[i, 0] = w2[i, 0] - dw2[i, 0]\n",
    "        for i in range(w2.shape[0]):\n",
    "            for j in range(1, w2.shape[1]):\n",
    "                w2[i, j] = w2[i, j] - (dw2[i, j] + (Lambda/m) * w2[i, j])\n",
    "        \n",
    "    model = { 'w1': w1, 'w2': w2}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "nn_input_dim = 3012# input layer dimensionality\n",
    "nn_output_dim = 10 # output layer dimensionality\n",
    "nn_hdim=30\n",
    "w1,w2=initialize_weights(nn_input_dim, nn_hdim,nn_output_dim)\n",
    "#gradient decent\n",
    "eta=0.01\n",
    "reg_lambda=0.01\n",
    "model=Network(w1,w2,trainRDD,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=trainRDD.map(lambda x:predict(model,x[0:-1])).collect()\n",
    "accuracy_score(trainRDD.map(lambda x:x[-1]).collect(), y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
