{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.43, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer=2\n",
    "output_layer=10\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "#gradient decent\n",
    "eta=0.00\n",
    "reg_lambda=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim_input, dim_hidden,dim_output):\n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    w1 = np.random.randn(dim_input, dim_hidden) / np.sqrt(dim_input)\n",
    "    b1 = np.zeros((1, dim_hidden))\n",
    "    w0 = np.random.randn(dim_hidden, dim_output) / np.sqrt(dim_hidden)\n",
    "    b0 = np.zeros((1, dim_output))\n",
    "    return w1,b1,w0,b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_scores = np.exp(z)\n",
    "    return (exp_scores / np.sum(exp_scores, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(W1,b1,W2,b2,X):\n",
    "#     z1 = X.dot(w1) \n",
    "#     a1 = np.tanh(z1)\n",
    "#     z2 = a1.dot(w0) + b0\n",
    "#     exp_scores = np.exp(z2)\n",
    "#     probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "#     #print(probs)\n",
    "#     return z1,a1,z2,probs\n",
    "    a1 = np.column_stack(( X))\n",
    "    print(np.shape(a1))\n",
    "    # hidden activations\n",
    "    z1 = a1.dot(W1)\n",
    "    a1 = np.tanh(z1)\n",
    "    #a1 = np.vstack(( sigmoid(z1)))\n",
    "\n",
    "    # output activations\n",
    "    z2 = a1.dot(W2)\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return z1,a1,z2,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(X,y,nn_hdim, num_passes, print_loss):\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    W1,b1,W2,b2=initialize_weights(nn_input_dim, nn_hdim,nn_output_dim)\n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0,num_passes):\n",
    "        #Forward propagation   \n",
    "        k=0\n",
    "        for x in X:\n",
    "            \n",
    "            #z1,a1,z2,probs=forward(W1,b1,W2,b2,x)\n",
    "        \n",
    "            dW1,db1,dW2,db2=feedforward(W1,b1,W2,b2,x,y[k])\n",
    "#             # Backpropagation\n",
    "#             delta3 = probs\n",
    "#             #print(len(x))\n",
    "#             delta3[range(len(x)-1), y[k]] -= 1\n",
    "#             dW2 = (a1.T).dot(delta3)\n",
    "#             db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "#             delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "#             m=np.reshape(x,(2,1))\n",
    "#             dW1 = np.dot(m, delta2)\n",
    "#             db1 = np.sum(delta2, axis=0)\n",
    "            k=k+1\n",
    "            # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "        W1 += -eta * dW1\n",
    "        b1 += -eta * db1\n",
    "        W2 += -eta * dW2\n",
    "        b2 += -eta * db2\n",
    "            \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            \n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model,X,y)))  \n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(model,X,y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1,a1,z2,probs=forward(W1,b1,W2,b2,X)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1,a1,z2,probs=forward(W1,b1,W2,b2,x)\n",
    "    print(np.shape(W1))\n",
    "    return np.argmax(probs,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(2, 114)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,114) and (2,40) not aligned: 114 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-372-b0b83709d3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Build a model with a 3-dimensional hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-365-aa27745afd96>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(X, y, nn_hdim, num_passes, print_loss)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss after iteration %i: %f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-146f2b641d0c>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[1;31m# Calculating the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorect_logprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-351-ce547c4612fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;31m# hidden activations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[1;31m#a1 = np.vstack(( sigmoid(z1)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,114) and (2,40) not aligned: 114 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "num_examples = len(X_train)\n",
    "model = build_model(X_train,y_train,40,100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feedforward(W1,b1,W2,b2,x,y):\n",
    "    a1 = np.column_stack(( x))\n",
    "    print(np.shape(a1))\n",
    "    # hidden activations\n",
    "    z1 = a1.dot(W1)\n",
    "    a1 = np.tanh(z1)\n",
    "    #a1 = np.vstack(( sigmoid(z1)))\n",
    "\n",
    "    # output activations\n",
    "    z2 = a1.dot(W2)\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    #back Propagation\n",
    "    delta3 = probs\n",
    "            #print(len(x))\n",
    "    delta3[range(len(x)-1), y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "    m=np.reshape(x,(2,1))\n",
    "    dW1 = np.dot(m, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    return dW1,db1,dW2,db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "y=np.array(([[1],[2]]))\n",
    "y_pred=[]\n",
    "for i in X_train:\n",
    "    y_pred.append(predict(model,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85964912280701755"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 40)\n"
     ]
    }
   ],
   "source": [
    "y_pred.append(predict(model,np.array([1,2])))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
